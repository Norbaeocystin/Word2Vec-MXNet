{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling cython code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from preprocessing import data_iterator_cython\n",
    "import logging\n",
    "import sys, random, time, math\n",
    "from collections import namedtuple\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip -O text8.gz && gzip -d text8.gz -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Text8Corpus(\"text8\")\n",
    "current_time = time.time()\n",
    "model = Word2Vec(iter=1, sg=1)\n",
    "model.build_vocab(corpus)\n",
    "print \"Building vocab took %s seconds\" % (time.time() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "batch_label = []\n",
    "batch_label_weight = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.5669488907\n",
      "Data prep took:  60.5673439503\n"
     ]
    }
   ],
   "source": [
    "current_time = time.time()\n",
    "job_batch, batch_size = [], 0\n",
    "for sent_idx, sentence in enumerate(corpus):\n",
    "    sentence_length = model._raw_word_count([sentence])\n",
    "\n",
    "    # can we fit this sentence into the existing job batch?\n",
    "    if batch_size + sentence_length <= model.batch_words:\n",
    "        # yes => add it to the current job\n",
    "        job_batch.append(sentence)\n",
    "        batch_size += sentence_length\n",
    "    else:\n",
    "        sents = data_iterator_cython(model, job_batch, model.alpha)\n",
    "        for sent in sents:\n",
    "            batch_data.append(sent[0])\n",
    "            batch_label.append(sent[1:])\n",
    "        job_batch[:] = []\n",
    "        batch_size = 0\n",
    "print time.time() - current_time\n",
    "print \"Data prep took: \", time.time() - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = mx.nd.array(batch_data)\n",
    "batch_label = mx.nd.array(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_weight = mx.nd.zeros((batch_data.shape[0], model.negative+1))\n",
    "target_weight[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = mx.nd.expand_dims(batch_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nd_iter = mx.io.NDArrayIter(data = {\"center_word\" : batch_data, \"target_words\": batch_label},\n",
    "                            label={ \"labels\":target_weight},\n",
    "                            batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_dim = model.negative\n",
    "vocab_size = len(model.wv.vocab)\n",
    "dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sym_makeloss(vocab_size, dim, batch_size):\n",
    "    labels = mx.sym.Variable('labels') #1 positive and k \"0\" labels\n",
    "    center_word = mx.sym.Variable('center_word')\n",
    "    target_words = mx.sym.Variable('target_words') # 1 target + k negative samples\n",
    "    center_vector = mx.sym.Embedding(data = center_word, input_dim = vocab_size,\n",
    "                                  output_dim = dim, name = 'syn0_embedding')\n",
    "    target_vectors = mx.sym.Embedding(data = target_words, input_dim = vocab_size,\n",
    "                                   output_dim = dim, name = 'syn1_embedding')\n",
    "    pred = mx.sym.batch_dot(target_vectors, center_vector, transpose_b=True)\n",
    "    sigmoid = mx.sym.sigmoid(mx.sym.flatten(pred))\n",
    "    loss = mx.sym.sum(labels * mx.sym.log(sigmoid) + (1 - labels) * mx.sym.log(1 - sigmoid), axis=1)\n",
    "    loss *= -1.0\n",
    "    loss_layer = mx.sym.MakeLoss(loss, normalization=\"batch\")\n",
    "    return loss_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_loss(label, pred):\n",
    "    return np.mean(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Already bound, ignoring bind()\n",
      "INFO:Epoch[0] Batch [1000]\tSpeed: 242752.60 samples/sec\tmean_loss=3.007094\n",
      "INFO:Epoch[0] Batch [2000]\tSpeed: 242771.79 samples/sec\tmean_loss=2.589738\n",
      "INFO:Epoch[0] Batch [3000]\tSpeed: 242706.89 samples/sec\tmean_loss=2.524906\n",
      "INFO:Epoch[0] Batch [4000]\tSpeed: 242616.23 samples/sec\tmean_loss=2.481047\n",
      "INFO:Epoch[0] Batch [5000]\tSpeed: 242585.36 samples/sec\tmean_loss=2.447668\n",
      "INFO:Epoch[0] Batch [6000]\tSpeed: 242328.63 samples/sec\tmean_loss=2.420793\n",
      "INFO:Epoch[0] Batch [7000]\tSpeed: 241763.66 samples/sec\tmean_loss=2.399166\n",
      "INFO:Epoch[0] Batch [8000]\tSpeed: 241167.39 samples/sec\tmean_loss=2.383484\n",
      "INFO:Epoch[0] Batch [9000]\tSpeed: 240646.68 samples/sec\tmean_loss=2.369594\n",
      "INFO:Epoch[0] Batch [10000]\tSpeed: 240549.45 samples/sec\tmean_loss=2.357569\n",
      "INFO:Epoch[0] Batch [11000]\tSpeed: 240155.86 samples/sec\tmean_loss=2.347496\n",
      "INFO:Epoch[0] Batch [12000]\tSpeed: 240275.52 samples/sec\tmean_loss=2.338892\n",
      "INFO:Epoch[0] Batch [13000]\tSpeed: 240080.05 samples/sec\tmean_loss=2.331839\n",
      "INFO:Epoch[0] Batch [14000]\tSpeed: 240244.78 samples/sec\tmean_loss=2.323067\n",
      "INFO:Epoch[0] Batch [15000]\tSpeed: 240009.82 samples/sec\tmean_loss=2.317843\n",
      "INFO:Epoch[0] Batch [16000]\tSpeed: 239957.21 samples/sec\tmean_loss=2.310637\n",
      "INFO:Epoch[0] Batch [17000]\tSpeed: 240011.29 samples/sec\tmean_loss=2.306468\n",
      "INFO:Epoch[0] Batch [18000]\tSpeed: 240494.34 samples/sec\tmean_loss=2.301567\n",
      "INFO:Epoch[0] Train-mean_loss=2.298560\n",
      "INFO:Epoch[0] Time cost=155.527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.532957077\n"
     ]
    }
   ],
   "source": [
    "nd_iter.reset()\n",
    "sym = get_sym_makeloss(vocab_size, dim, batch_size)\n",
    "network = mx.mod.Module(sym, data_names=(\"center_word\", \"target_words\",), label_names=(\"labels\",),context=mx.gpu())\n",
    "network.bind(data_shapes=nd_iter.provide_data, label_shapes=nd_iter.provide_label)\n",
    "current_time = time.time()\n",
    "network.fit(nd_iter, num_epoch=1,optimizer='adam',\n",
    "            eval_metric=mx.metric.CustomMetric(mean_loss),\n",
    "            optimizer_params={'learning_rate': .001},\n",
    "            batch_end_callback=mx.callback.Speedometer(batch_size, 1000),\n",
    "            initializer=mx.initializer.Uniform(scale=.01))\n",
    "print time.time() - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vecs = network.get_params()[0][\"syn0_embedding_weight\"].asnumpy()\n",
    "all_vecs = normalize(all_vecs, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.syn0 = all_vecs\n",
    "model.wv.syn0norm = all_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'planet', 0.84510737657547),\n",
       " (u'moon', 0.8245026469230652),\n",
       " (u'sun', 0.805547833442688),\n",
       " (u'jupiter', 0.7573765516281128),\n",
       " (u'galaxy', 0.7457337379455566),\n",
       " (u'orbiting', 0.7420254945755005),\n",
       " (u'uranus', 0.7418688535690308),\n",
       " (u'solar', 0.7407880425453186),\n",
       " (u'planets', 0.7369117736816406),\n",
       " (u'celestial', 0.7360233068466187)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"earth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
