{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling cython code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from preprocessing import data_iterator_cython\n",
    "import logging\n",
    "import sys, random, time, math\n",
    "from collections import namedtuple\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip -O text8.gz && gzip -d text8.gz -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Text8Corpus(\"text8\")\n",
    "current_time = time.time()\n",
    "model = Word2Vec(iter=1, sg=1)\n",
    "model.build_vocab(corpus)\n",
    "print \"Building vocab took %s seconds\" % (time.time() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "batch_label = []\n",
    "batch_label_weight = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.5669488907\n",
      "Data prep took:  60.5673439503\n"
     ]
    }
   ],
   "source": [
    "current_time = time.time()\n",
    "job_batch, batch_size = [], 0\n",
    "for sent_idx, sentence in enumerate(corpus):\n",
    "    sentence_length = model._raw_word_count([sentence])\n",
    "\n",
    "    # can we fit this sentence into the existing job batch?\n",
    "    if batch_size + sentence_length <= model.batch_words:\n",
    "        # yes => add it to the current job\n",
    "        job_batch.append(sentence)\n",
    "        batch_size += sentence_length\n",
    "    else:\n",
    "        sents = data_iterator_cython(model, job_batch, model.alpha)\n",
    "        for sent in sents:\n",
    "            batch_data.append(sent[0])\n",
    "            batch_label.append(sent[1:])\n",
    "        job_batch[:] = []\n",
    "        batch_size = 0\n",
    "print time.time() - current_time\n",
    "print \"Data prep took: \", time.time() - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = mx.nd.array(batch_data)\n",
    "batch_label = mx.nd.array(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_weight = mx.nd.zeros((batch_data.shape[0], model.negative+1))\n",
    "target_weight[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = mx.nd.expand_dims(batch_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nd_iter = mx.io.NDArrayIter(data = {\"center_word\" : batch_data, \"target_words\": batch_label},\n",
    "                            label={ \"labels\":target_weight},\n",
    "                            batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_dim = model.negative\n",
    "vocab_size = len(model.wv.vocab)\n",
    "dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sym_makeloss(vocab_size, dim, batch_size):\n",
    "    labels = mx.sym.Variable('labels') #1 positive and k \"0\" labels\n",
    "    center_word = mx.sym.Variable('center_word')\n",
    "    target_words = mx.sym.Variable('target_words') # 1 target + k negative samples\n",
    "    center_vector = mx.sym.Embedding(data = center_word, input_dim = vocab_size,\n",
    "                                  output_dim = dim, name = 'syn0_embedding')\n",
    "    target_vectors = mx.sym.Embedding(data = target_words, input_dim = vocab_size,\n",
    "                                   output_dim = dim, name = 'syn1_embedding')\n",
    "    pred = mx.sym.batch_dot(target_vectors, center_vector, transpose_b=True)\n",
    "    sigmoid = mx.sym.sigmoid(mx.sym.flatten(pred))\n",
    "    loss = mx.sym.sum(labels * mx.sym.log(sigmoid) + (1 - labels) * mx.sym.log(1 - sigmoid), axis=1)\n",
    "    loss *= -1.0\n",
    "    loss_layer = mx.sym.MakeLoss(loss, normalization=\"batch\")\n",
    "    return loss_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_loss(label, pred):\n",
    "    return np.mean(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Already bound, ignoring bind()\n",
      "INFO:Epoch[0] Batch [1000]\tSpeed: 67514.10 samples/sec\tmean_loss=3.324762\n",
      "INFO:Epoch[0] Batch [2000]\tSpeed: 68750.90 samples/sec\tmean_loss=2.749573\n",
      "INFO:Epoch[0] Batch [3000]\tSpeed: 68509.34 samples/sec\tmean_loss=2.640605\n",
      "INFO:Epoch[0] Batch [4000]\tSpeed: 68214.37 samples/sec\tmean_loss=2.589391\n",
      "INFO:Epoch[0] Batch [5000]\tSpeed: 68182.27 samples/sec\tmean_loss=2.556170\n",
      "INFO:Epoch[0] Batch [6000]\tSpeed: 68160.76 samples/sec\tmean_loss=2.530014\n",
      "INFO:Epoch[0] Batch [7000]\tSpeed: 68195.24 samples/sec\tmean_loss=2.510201\n",
      "INFO:Epoch[0] Batch [8000]\tSpeed: 68133.54 samples/sec\tmean_loss=2.492381\n",
      "INFO:Epoch[0] Batch [9000]\tSpeed: 68027.80 samples/sec\tmean_loss=2.477695\n",
      "INFO:Epoch[0] Batch [10000]\tSpeed: 67952.16 samples/sec\tmean_loss=2.463432\n",
      "INFO:Epoch[0] Batch [11000]\tSpeed: 67983.85 samples/sec\tmean_loss=2.451389\n",
      "INFO:Epoch[0] Batch [12000]\tSpeed: 68103.80 samples/sec\tmean_loss=2.442641\n",
      "INFO:Epoch[0] Batch [13000]\tSpeed: 67980.46 samples/sec\tmean_loss=2.433064\n",
      "INFO:Epoch[0] Batch [14000]\tSpeed: 67730.39 samples/sec\tmean_loss=2.424111\n",
      "INFO:Epoch[0] Batch [15000]\tSpeed: 67852.51 samples/sec\tmean_loss=2.414362\n",
      "INFO:Epoch[0] Batch [16000]\tSpeed: 67830.14 samples/sec\tmean_loss=2.408149\n",
      "INFO:Epoch[0] Batch [17000]\tSpeed: 67816.72 samples/sec\tmean_loss=2.402137\n",
      "INFO:Epoch[0] Batch [18000]\tSpeed: 67779.51 samples/sec\tmean_loss=2.396613\n",
      "INFO:Epoch[0] Batch [19000]\tSpeed: 67794.72 samples/sec\tmean_loss=2.390748\n",
      "INFO:Epoch[0] Batch [20000]\tSpeed: 67896.24 samples/sec\tmean_loss=2.385985\n",
      "INFO:Epoch[0] Batch [21000]\tSpeed: 67810.95 samples/sec\tmean_loss=2.382373\n",
      "INFO:Epoch[0] Batch [22000]\tSpeed: 67743.80 samples/sec\tmean_loss=2.377004\n",
      "INFO:Epoch[0] Batch [23000]\tSpeed: 67762.66 samples/sec\tmean_loss=2.372241\n",
      "INFO:Epoch[0] Batch [24000]\tSpeed: 67720.10 samples/sec\tmean_loss=2.369666\n",
      "INFO:Epoch[0] Batch [25000]\tSpeed: 67725.52 samples/sec\tmean_loss=2.366825\n",
      "INFO:Epoch[0] Batch [26000]\tSpeed: 67697.73 samples/sec\tmean_loss=2.361496\n",
      "INFO:Epoch[0] Batch [27000]\tSpeed: 67686.33 samples/sec\tmean_loss=2.358756\n",
      "INFO:Epoch[0] Batch [28000]\tSpeed: 67740.80 samples/sec\tmean_loss=2.354761\n",
      "INFO:Epoch[0] Batch [29000]\tSpeed: 67735.33 samples/sec\tmean_loss=2.355581\n",
      "INFO:Epoch[0] Batch [30000]\tSpeed: 67715.76 samples/sec\tmean_loss=2.351064\n",
      "INFO:Epoch[0] Batch [31000]\tSpeed: 67751.33 samples/sec\tmean_loss=2.347874\n",
      "INFO:Epoch[0] Batch [32000]\tSpeed: 67656.19 samples/sec\tmean_loss=2.343565\n",
      "INFO:Epoch[0] Batch [33000]\tSpeed: 67759.25 samples/sec\tmean_loss=2.341243\n",
      "INFO:Epoch[0] Batch [34000]\tSpeed: 67711.36 samples/sec\tmean_loss=2.338863\n",
      "INFO:Epoch[0] Batch [35000]\tSpeed: 67727.28 samples/sec\tmean_loss=2.337944\n",
      "INFO:Epoch[0] Batch [36000]\tSpeed: 67809.66 samples/sec\tmean_loss=2.335212\n",
      "INFO:Epoch[0] Batch [37000]\tSpeed: 67680.95 samples/sec\tmean_loss=2.333533\n",
      "INFO:Epoch[0] Batch [38000]\tSpeed: 67876.51 samples/sec\tmean_loss=2.332975\n",
      "INFO:Epoch[0] Batch [39000]\tSpeed: 67886.54 samples/sec\tmean_loss=2.332215\n",
      "INFO:Epoch[0] Batch [40000]\tSpeed: 67921.23 samples/sec\tmean_loss=2.327820\n",
      "INFO:Epoch[0] Batch [41000]\tSpeed: 67713.19 samples/sec\tmean_loss=2.326940\n",
      "INFO:Epoch[0] Batch [42000]\tSpeed: 67681.45 samples/sec\tmean_loss=2.325459\n",
      "INFO:Epoch[0] Batch [43000]\tSpeed: 67913.52 samples/sec\tmean_loss=2.323512\n",
      "INFO:Epoch[0] Batch [44000]\tSpeed: 67631.49 samples/sec\tmean_loss=2.321704\n",
      "INFO:Epoch[0] Batch [45000]\tSpeed: 67738.53 samples/sec\tmean_loss=2.321032\n",
      "INFO:Epoch[0] Batch [46000]\tSpeed: 67643.81 samples/sec\tmean_loss=2.318252\n",
      "INFO:Epoch[0] Batch [47000]\tSpeed: 67939.77 samples/sec\tmean_loss=2.316331\n",
      "INFO:Epoch[0] Batch [48000]\tSpeed: 67909.84 samples/sec\tmean_loss=2.314698\n",
      "INFO:Epoch[0] Batch [49000]\tSpeed: 67648.43 samples/sec\tmean_loss=2.315520\n",
      "INFO:Epoch[0] Batch [50000]\tSpeed: 67928.81 samples/sec\tmean_loss=2.311994\n",
      "INFO:Epoch[0] Batch [51000]\tSpeed: 67713.57 samples/sec\tmean_loss=2.311395\n",
      "INFO:Epoch[0] Batch [52000]\tSpeed: 67876.75 samples/sec\tmean_loss=2.311811\n",
      "INFO:Epoch[0] Batch [53000]\tSpeed: 67721.82 samples/sec\tmean_loss=2.309732\n",
      "INFO:Epoch[0] Batch [54000]\tSpeed: 67903.75 samples/sec\tmean_loss=2.308115\n",
      "INFO:Epoch[0] Batch [55000]\tSpeed: 67938.74 samples/sec\tmean_loss=2.310613\n",
      "INFO:Epoch[0] Batch [56000]\tSpeed: 67961.86 samples/sec\tmean_loss=2.307433\n",
      "INFO:Epoch[0] Batch [57000]\tSpeed: 67958.09 samples/sec\tmean_loss=2.304702\n",
      "INFO:Epoch[0] Batch [58000]\tSpeed: 67836.98 samples/sec\tmean_loss=2.303065\n",
      "INFO:Epoch[0] Batch [59000]\tSpeed: 67706.65 samples/sec\tmean_loss=2.305031\n",
      "INFO:Epoch[0] Batch [60000]\tSpeed: 67841.63 samples/sec\tmean_loss=2.303309\n",
      "INFO:Epoch[0] Batch [61000]\tSpeed: 67702.88 samples/sec\tmean_loss=2.301929\n",
      "INFO:Epoch[0] Batch [62000]\tSpeed: 67615.42 samples/sec\tmean_loss=2.300102\n",
      "INFO:Epoch[0] Batch [63000]\tSpeed: 67681.34 samples/sec\tmean_loss=2.300638\n",
      "INFO:Epoch[0] Batch [64000]\tSpeed: 67955.00 samples/sec\tmean_loss=2.298462\n",
      "INFO:Epoch[0] Batch [65000]\tSpeed: 67890.57 samples/sec\tmean_loss=2.296858\n"
     ]
    }
   ],
   "source": [
    "nd_iter.reset()\n",
    "sym = get_sym_makeloss(vocab_size, dim, batch_size)\n",
    "network = mx.mod.Module(sym, data_names=(\"center_word\", \"target_words\",), label_names=(\"labels\",),context=mx.gpu())\n",
    "network.bind(data_shapes=nd_iter.provide_data, label_shapes=nd_iter.provide_label)\n",
    "current_time = time.time()\n",
    "network.fit(nd_iter, num_epoch=1,optimizer='adam',\n",
    "            eval_metric=mx.metric.CustomMetric(mean_loss),\n",
    "            optimizer_params={'learning_rate': .001},\n",
    "            batch_end_callback=mx.callback.Speedometer(batch_size, 1000),\n",
    "            initializer=mx.initializer.Uniform(scale=.01))\n",
    "print time.time() - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vecs = network.get_params()[0][\"syn0_embedding_weight\"].asnumpy()\n",
    "all_vecs = normalize(all_vecs, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.syn0 = all_vecs\n",
    "model.wv.syn0norm = all_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'driver', 0.7786462306976318),\n",
       " (u'motorcycle', 0.7644623517990112),\n",
       " (u'airplane', 0.7162174582481384),\n",
       " (u'taxi', 0.7073050141334534),\n",
       " (u'supercar', 0.6959617137908936),\n",
       " (u'jumbo', 0.6949251294136047),\n",
       " (u'cars', 0.6885921359062195),\n",
       " (u'racing', 0.6778541207313538),\n",
       " (u'truck', 0.6697883605957031),\n",
       " (u'automobiles', 0.6633998155593872)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
